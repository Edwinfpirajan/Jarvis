# üéôÔ∏è Gu√≠a de Setup Local - Jarvis 100% Sin IA en la Nube

Esta gu√≠a te ayudar√° a configurar Jarvis para que funcione **completamente local** sin depender de ning√∫n servicio de IA en la nube (OpenAI, etc.).

---

## üéØ ¬øQu√© significa "100% Local"?

- ‚úÖ **STT (Speech-to-Text)**: Whisper.cpp ejecut√°ndose en tu PC
- ‚úÖ **LLM (Language Model)**: Ollama con modelos locales
- ‚úÖ **TTS (Text-to-Speech)**: Piper generando voz en tu PC
- ‚úÖ **Sin conexi√≥n a internet requerida** para funcionalidades principales
- ‚úÖ **Sin API keys** de servicios externos
- ‚úÖ **Privacidad total**: Ning√∫n dato sale de tu computadora

---

## üöÄ Setup R√°pido (Recomendado)

### Opci√≥n A: Script Autom√°tico

```powershell
# Ejecutar script maestro de instalaci√≥n
.\scripts\setup_local.ps1
```

Este script instalar√° autom√°ticamente:
- Piper TTS + voz en espa√±ol
- Modelos de Whisper.cpp
- Verificar√° Ollama
- Configurar√° Jarvis para modo local

### Opci√≥n B: Instalaci√≥n Manual Paso a Paso

Sigue las secciones a continuaci√≥n.

---

## üì¶ Requisitos Previos

- **Windows 10/11** (x64)
- **Go 1.22+** instalado
- **PowerShell 5.1+**
- **~5 GB de espacio en disco** (para modelos)
- **8 GB+ RAM recomendado**

---

## üîß Instalaci√≥n Componente por Componente

### 1Ô∏è‚É£ Ollama (LLM Local)

**¬øQu√© hace?**: Procesa tus comandos de voz y decide qu√© acci√≥n ejecutar.

#### Instalaci√≥n:

```powershell
# Opci√≥n A: Descarga desde la web
# Visita: https://ollama.ai/download
# Descarga el instalador para Windows y ejec√∫talo

# Opci√≥n B: Con Winget (Windows Package Manager)
winget install Ollama.Ollama
```

#### Configuraci√≥n:

```powershell
# 1. Iniciar servidor Ollama (d√©jalo corriendo en segundo plano)
ollama serve

# 2. En otra terminal, descargar modelo (3 GB aproximadamente)
ollama pull llama3.2:3b

# Verificar instalaci√≥n
ollama list
```

**Modelos alternativos**:
- `llama3.2:1b` - M√°s r√°pido, menos preciso (1 GB)
- `mistral:7b` - M√°s potente, m√°s lento (4 GB)
- `phi3:mini` - Balance intermedio (2 GB)

---

### 2Ô∏è‚É£ Piper (TTS Local)

**¬øQu√© hace?**: Convierte texto a voz para que Jarvis "hable".

#### Instalaci√≥n Autom√°tica:

```powershell
# Instalar Piper
.\scripts\install_piper.ps1

# Descargar voz en espa√±ol (Espa√±a)
.\scripts\download_voices.ps1

# O voz de M√©xico
.\scripts\download_voices.ps1 -Language es_MX -Voice ald -Quality medium
```

#### Instalaci√≥n Manual:

```powershell
# 1. Crear directorio
mkdir -p bin\piper

# 2. Descargar desde GitHub
# Visita: https://github.com/rhasspy/piper/releases
# Descarga: piper_windows_amd64.zip
# Extrae el contenido en: bin\piper\

# 3. Descargar modelo de voz
mkdir -p assets\voices\piper

# URL del modelo (copia en navegador):
# https://huggingface.co/rhasspy/piper-voices/resolve/main/es/es_ES/davefx/medium/es_ES-davefx-medium.onnx
# Guardar en: assets\voices\piper\es_ES-davefx-medium.onnx

# Tambi√©n descargar el archivo JSON:
# https://huggingface.co/rhasspy/piper-voices/resolve/main/es/es_ES/davefx/medium/es_ES-davefx-medium.onnx.json
# Guardar en: assets\voices\piper\es_ES-davefx-medium.onnx.json
```

#### Probar Piper:

```powershell
echo "Hola, soy Jarvis" | .\bin\piper\piper.exe --model .\assets\voices\piper\es_ES-davefx-medium.onnx --output_file test.wav
```

---

### 3Ô∏è‚É£ Whisper.cpp (STT Local)

**¬øQu√© hace?**: Convierte tu voz en texto.

#### Instalaci√≥n Autom√°tica (Solo Modelos):

```powershell
# Descargar modelo base (142 MB)
.\scripts\install_whisper.ps1 -Model base

# O modelo peque√±o para mejor precisi√≥n (466 MB)
.\scripts\install_whisper.ps1 -Model small
```

#### Instalaci√≥n del Binario:

**Opci√≥n A - Descarga Precompilada (Recomendada)**:

1. Visita: [Whisper.cpp Releases](https://github.com/ggerganov/whisper.cpp/releases)
2. Descarga: `whisper-bin-x64.zip` (Windows)
3. Extrae `main.exe` en: `bin\whisper\`

**Opci√≥n B - Compilar desde Fuente (Avanzado)**:

```powershell
# Requiere: Visual Studio 2022 + CMake + Git

git clone https://github.com/ggerganov/whisper.cpp.git
cd whisper.cpp
mkdir build
cd build
cmake ..
cmake --build . --config Release

# Copiar ejecutable
copy bin\Release\main.exe ..\..\..\bin\whisper\main.exe
```

#### Probar Whisper:

```powershell
# Grabar un audio WAV y probarlo
.\bin\whisper\main.exe -m .\assets\models\whisper\ggml-base.bin -f test.wav
```

---

## ‚öôÔ∏è Configuraci√≥n de Jarvis

### Configuraci√≥n Ya Aplicada

Tu archivo [config/jarvis.config.yaml](config/jarvis.config.yaml) ya est√° configurado para modo local:

```yaml
stt:
  provider: "whisper"  # ‚úÖ Local
  whisper:
    binary_path: "./bin/whisper/main.exe"
    model_path: "./assets/models/whisper/ggml-base.bin"
    language: "es"

llm:
  provider: "ollama"   # ‚úÖ Local
  ollama:
    url: "http://localhost:11434"
    model: "llama3.2:3b"

tts:
  provider: "piper"    # ‚úÖ Local
  piper:
    binary_path: "./bin/piper/piper.exe"
    model_path: "./assets/voices/piper/es_ES-davefx-medium.onnx"
    speed: 1.0
```

### Configuraci√≥n H√≠brida (Opcional)

Si quieres **fallback** a OpenAI cuando los servicios locales fallen:

```yaml
stt:
  provider: "whisper"  # Solo local

llm:
  provider: "auto"     # Intenta Ollama ‚Üí OpenAI

tts:
  provider: "auto"     # Intenta Piper ‚Üí OpenAI
```

---

## üèÉ Ejecutar Jarvis

### 1. Iniciar Ollama (si no est√° corriendo)

```powershell
# En una terminal separada (d√©jala abierta)
ollama serve
```

### 2. Compilar Jarvis

```powershell
# Desde la ra√≠z del proyecto
go build -o jarvis.exe ./cmd/jarvis
```

### 3. Ejecutar Jarvis

```powershell
# Modo interactivo (texto)
.\jarvis.exe

# Modo test
.\jarvis.exe -test -command "crea un clip"

# Modo voz (requiere audio configurado)
.\jarvis.exe -voice
```

---

## üß™ Verificar que Todo Funciona

### Checklist Pre-vuelo:

```powershell
# 1. Verificar Ollama
ollama list
# Debe mostrar: llama3.2:3b

# 2. Verificar Ollama est√° corriendo
Invoke-WebRequest http://localhost:11434/api/version
# Debe responder con versi√≥n

# 3. Verificar Piper
.\bin\piper\piper.exe --version
# Debe mostrar versi√≥n de Piper

# 4. Verificar voz de Piper
dir assets\voices\piper\*.onnx
# Debe listar: es_ES-davefx-medium.onnx

# 5. Verificar Whisper (si compilaste)
.\bin\whisper\main.exe --help
# Debe mostrar opciones

# 6. Verificar modelos Whisper
dir assets\models\whisper\*.bin
# Debe listar: ggml-base.bin
```

---

## üìä Comparativa: Local vs OpenAI

| Aspecto | Local | OpenAI |
|---------|-------|--------|
| **Costo** | Gratis | Pago por uso |
| **Privacidad** | Total | Env√≠a datos a la nube |
| **Velocidad** | Depende de tu PC | R√°pido (servidores potentes) |
| **Conexi√≥n** | No requerida | Requiere internet |
| **Calidad STT** | Muy buena | Excelente |
| **Calidad TTS** | Buena | Excelente |
| **Calidad LLM** | Buena (3B params) | Excelente (70B+ params) |
| **Setup** | Complejo | Trivial (solo API key) |

---

## üêõ Soluci√≥n de Problemas

### Problema: "Ollama not running"

```powershell
# Soluci√≥n: Iniciar Ollama
ollama serve
```

### Problema: "Piper binary not found"

```powershell
# Verificar ruta
dir bin\piper\piper.exe

# Re-ejecutar instalaci√≥n
.\scripts\install_piper.ps1
```

### Problema: "Whisper model not found"

```powershell
# Descargar modelo
.\scripts\install_whisper.ps1 -Model base
```

### Problema: "STT failed: file not found"

**Causa**: Whisper.cpp no instalado (requiere compilaci√≥n manual)

**Soluci√≥n Temporal**: Usa OpenAI STT mientras tanto

```yaml
stt:
  provider: "openai"
  openai:
    api_key: "${OPENAI_API_KEY}"
```

### Problema: "LLM timeout"

**Causa**: Modelo Ollama muy grande para tu PC

**Soluci√≥n**: Usar modelo m√°s peque√±o

```powershell
# Descargar modelo m√°s ligero
ollama pull llama3.2:1b

# Actualizar config
# llm.ollama.model: "llama3.2:1b"
```

---

## üéØ Optimizaciones de Rendimiento

### Para PCs de gama baja:

```yaml
llm:
  ollama:
    model: "llama3.2:1b"  # Modelo m√°s peque√±o

stt:
  whisper:
    model_path: "./assets/models/whisper/ggml-tiny.bin"  # Modelo m√°s r√°pido

tts:
  piper:
    model_path: "./assets/voices/piper/es_ES-sharvard-low.onnx"  # Voz de baja calidad pero r√°pida
```

### Para PCs potentes:

```yaml
llm:
  ollama:
    model: "mistral:7b"  # Modelo m√°s inteligente

stt:
  whisper:
    model_path: "./assets/models/whisper/ggml-medium.bin"  # Mejor precisi√≥n

tts:
  piper:
    model_path: "./assets/voices/piper/es_ES-davefx-high.onnx"  # Mejor calidad de voz
```

---

## üìö Recursos Adicionales

- [Ollama Models Library](https://ollama.ai/library)
- [Piper Voice Samples](https://rhasspy.github.io/piper-samples/)
- [Whisper.cpp GitHub](https://github.com/ggerganov/whisper.cpp)
- [Jarvis README](README.md)

---

## ü§ù ¬øNecesitas Ayuda?

- **Issues**: Abre un issue en GitHub
- **Discord**: [Enlace al servidor] (si existe)
- **Email**: [tu email]

---

**¬°Disfruta de tu asistente de voz 100% local!** üéôÔ∏èüöÄ
